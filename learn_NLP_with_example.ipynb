{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A46dpMKWlDp",
        "outputId": "70b581a2-1786-40cc-c079-e2aa34e52518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 675
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"\"\" Iran is facing an unprecedented environmental crisis, with dwindling water resources, rapid deforestation, desertification, overgrazing of rangelands, and pollution that chokes its cities. This problem, if left unchecked, threatens not only Iran, but also the stability of the region and the world. \n",
        "Iran’s environmental status these days include air pollution, pesticide pollution, soil depletion and erosion, water scarcity and pollution, natural resource loss, lack of appropriate waste management, lead poisoning, and desertification. \n",
        "Environmental policy regulations and how to implement is described under the previous regime and the clerical regime, however, \n",
        "no implementation of those regulations exist. \n",
        "Iran is challenged with many interrelated political, social and natural crisis including environmental degradation, unemployment, poverty, and population growth. Sustainability is being undermined in every aspects of environmental issues at the cost of future generations. \n",
        "Iran with a population of over 80 million, the country is having difficulty in maintaining its current infrastructure, housing, food, and educational facilities. \n",
        "Population growth leads to increased demand for infrastructure and resources. The regime additionally is faced with extreme uprisings of various sectors of the society demanding Iran’s  “Freedom and Water”. \"\"\"\n",
        "sentences = nltk.sent_tokenize(text1)\n",
        "#nltk.sent_tokenize is a function from the Natural Language Toolkit (NLTK) library in Python that is used to split a text into sentences. It is part of the NLTK's tokenize module, which provides several functions for splitting texts into tokens, such as words or sentences."
      ],
      "metadata": {
        "id": "U5DRNwZEYCbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMakxXtuYyKU",
        "outputId": "946654ae-922b-484c-feb5-fe9c17834b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Iran is facing an unprecedented environmental crisis, with dwindling water resources, rapid deforestation, desertification, overgrazing of rangelands, and pollution that chokes its cities.', 'This problem, if left unchecked, threatens not only Iran, but also the stability of the region and the world.', 'Iran’s environmental status these days include air pollution, pesticide pollution, soil depletion and erosion, water scarcity and pollution, natural resource loss, lack of appropriate waste management, lead poisoning, and desertification.', 'Environmental policy regulations and how to implement is described under the previous regime and the clerical regime, however, \\nno implementation of those regulations exist.', 'Iran is challenged with many interrelated political, social and natural crisis including environmental degradation, unemployment, poverty, and population growth.', 'Sustainability is being undermined in every aspects of environmental issues at the cost of future generations.', 'Iran with a population of over 80 million, the country is having difficulty in maintaining its current infrastructure, housing, food, and educational facilities.', 'Population growth leads to increased demand for infrastructure and resources.', 'The regime additionally is faced with extreme uprisings of various sectors of the society demanding Iran’s  “Freedom and Water”.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or we can use word tokenize\n",
        "words = nltk.word_tokenize(text1)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zetpP2BD4lqD",
        "outputId": "92801676-c098-40a8-b0f7-a4cffcf2e07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Iran',\n",
              " 'is',\n",
              " 'facing',\n",
              " 'an',\n",
              " 'unprecedented',\n",
              " 'environmental',\n",
              " 'crisis',\n",
              " ',',\n",
              " 'with',\n",
              " 'dwindling',\n",
              " 'water',\n",
              " 'resources',\n",
              " ',',\n",
              " 'rapid',\n",
              " 'deforestation',\n",
              " ',',\n",
              " 'desertification',\n",
              " ',',\n",
              " 'overgrazing',\n",
              " 'of',\n",
              " 'rangelands',\n",
              " ',',\n",
              " 'and',\n",
              " 'pollution',\n",
              " 'that',\n",
              " 'chokes',\n",
              " 'its',\n",
              " 'cities',\n",
              " '.',\n",
              " 'This',\n",
              " 'problem',\n",
              " ',',\n",
              " 'if',\n",
              " 'left',\n",
              " 'unchecked',\n",
              " ',',\n",
              " 'threatens',\n",
              " 'not',\n",
              " 'only',\n",
              " 'Iran',\n",
              " ',',\n",
              " 'but',\n",
              " 'also',\n",
              " 'the',\n",
              " 'stability',\n",
              " 'of',\n",
              " 'the',\n",
              " 'region',\n",
              " 'and',\n",
              " 'the',\n",
              " 'world',\n",
              " '.',\n",
              " 'Iran',\n",
              " '’',\n",
              " 's',\n",
              " 'environmental',\n",
              " 'status',\n",
              " 'these',\n",
              " 'days',\n",
              " 'include',\n",
              " 'air',\n",
              " 'pollution',\n",
              " ',',\n",
              " 'pesticide',\n",
              " 'pollution',\n",
              " ',',\n",
              " 'soil',\n",
              " 'depletion',\n",
              " 'and',\n",
              " 'erosion',\n",
              " ',',\n",
              " 'water',\n",
              " 'scarcity',\n",
              " 'and',\n",
              " 'pollution',\n",
              " ',',\n",
              " 'natural',\n",
              " 'resource',\n",
              " 'loss',\n",
              " ',',\n",
              " 'lack',\n",
              " 'of',\n",
              " 'appropriate',\n",
              " 'waste',\n",
              " 'management',\n",
              " ',',\n",
              " 'lead',\n",
              " 'poisoning',\n",
              " ',',\n",
              " 'and',\n",
              " 'desertification',\n",
              " '.',\n",
              " 'Environmental',\n",
              " 'policy',\n",
              " 'regulations',\n",
              " 'and',\n",
              " 'how',\n",
              " 'to',\n",
              " 'implement',\n",
              " 'is',\n",
              " 'described',\n",
              " 'under',\n",
              " 'the',\n",
              " 'previous',\n",
              " 'regime',\n",
              " 'and',\n",
              " 'the',\n",
              " 'clerical',\n",
              " 'regime',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'no',\n",
              " 'implementation',\n",
              " 'of',\n",
              " 'those',\n",
              " 'regulations',\n",
              " 'exist',\n",
              " '.',\n",
              " 'Iran',\n",
              " 'is',\n",
              " 'challenged',\n",
              " 'with',\n",
              " 'many',\n",
              " 'interrelated',\n",
              " 'political',\n",
              " ',',\n",
              " 'social',\n",
              " 'and',\n",
              " 'natural',\n",
              " 'crisis',\n",
              " 'including',\n",
              " 'environmental',\n",
              " 'degradation',\n",
              " ',',\n",
              " 'unemployment',\n",
              " ',',\n",
              " 'poverty',\n",
              " ',',\n",
              " 'and',\n",
              " 'population',\n",
              " 'growth',\n",
              " '.',\n",
              " 'Sustainability',\n",
              " 'is',\n",
              " 'being',\n",
              " 'undermined',\n",
              " 'in',\n",
              " 'every',\n",
              " 'aspects',\n",
              " 'of',\n",
              " 'environmental',\n",
              " 'issues',\n",
              " 'at',\n",
              " 'the',\n",
              " 'cost',\n",
              " 'of',\n",
              " 'future',\n",
              " 'generations',\n",
              " '.',\n",
              " 'Iran',\n",
              " 'with',\n",
              " 'a',\n",
              " 'population',\n",
              " 'of',\n",
              " 'over',\n",
              " '80',\n",
              " 'million',\n",
              " ',',\n",
              " 'the',\n",
              " 'country',\n",
              " 'is',\n",
              " 'having',\n",
              " 'difficulty',\n",
              " 'in',\n",
              " 'maintaining',\n",
              " 'its',\n",
              " 'current',\n",
              " 'infrastructure',\n",
              " ',',\n",
              " 'housing',\n",
              " ',',\n",
              " 'food',\n",
              " ',',\n",
              " 'and',\n",
              " 'educational',\n",
              " 'facilities',\n",
              " '.',\n",
              " 'Population',\n",
              " 'growth',\n",
              " 'leads',\n",
              " 'to',\n",
              " 'increased',\n",
              " 'demand',\n",
              " 'for',\n",
              " 'infrastructure',\n",
              " 'and',\n",
              " 'resources',\n",
              " '.',\n",
              " 'The',\n",
              " 'regime',\n",
              " 'additionally',\n",
              " 'is',\n",
              " 'faced',\n",
              " 'with',\n",
              " 'extreme',\n",
              " 'uprisings',\n",
              " 'of',\n",
              " 'various',\n",
              " 'sectors',\n",
              " 'of',\n",
              " 'the',\n",
              " 'society',\n",
              " 'demanding',\n",
              " 'Iran',\n",
              " '’',\n",
              " 's',\n",
              " '“',\n",
              " 'Freedom',\n",
              " 'and',\n",
              " 'Water',\n",
              " '”',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 678
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y87QiydRYwvX",
        "outputId": "9937a6fe-c042-4830-9ea1-f742baa5d310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(text1)"
      ],
      "metadata": {
        "id": "M7doXfJUZmEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install hazm"
      ],
      "metadata": {
        "id": "RdiI65TQ6Dbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *"
      ],
      "metadata": {
        "id": "ndQ41v6k58oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_persian = \"\"\"این روزها دیگر کمتر به ارزش و تقدس درخت ها توجه می شود. می گویند اوضاع مان در تولید کاغذ خوب نیست اما اینجا زمین است، همان سیاره ای که خاطرات و سرگذشتمان در آن رقم خورده. همان جا که روی آن متولد شدیم، رشد کردیم، بزرگ شدیم، درس خواندیم، برای هر لحظه زندگی نقشه کشیدیم، فکر کردیم و برای فرزندانمان برنامه ریختیم. زمین همین جایی است که قرار است سال های سال و شاید قرن های قرن خانه نسل های بعد از ما باشد و آن ها هم بر همین خاکی زندگی کنند و در همین هوا تنفس کنند و از همین منابع زیست محیطی استفاده کنند که ما می کنیم. اما این خانه بزرگ ما، احتیاج به محافظت دارد آن هم از طرف همه ما. تا به حال شده از خودتان بپرسید این همه کاغذی که هرروز به شکل های مختلف؛ از روزنامه و دفتر و کتاب گرفته، تا دستمال کاغذی و بسته بندی های جور واجور، در دسترسمان است از کجا و چطور تأمین می شود؟ و تا به حال شده به حجم درخت هایی که به خاطر مصارف بی دلیل ما قطع می شود و ضربه ای که به اکوسیستم زده می شود، فکر کنید؟این روزها دیگر کمتر به ارزش و تقدس درخت ها توجه می شود. می گویند اوضاع مان در تولید کاغذ خوب نیست اما مصرف مان خیلی زیاد و گاهی هم غیر منطقی است. می گویند سالانه بیش از یک میلیون و 500 هزار تن کاغذ در کشور مصرف می شود. می گویند برای تولید هر تُن کاغذ باید ۱۷ اصله درخت را قطع کرد و این یعنی قطع سالانه 25 میلیون و 500 هزار اصله درخت. علاوه بر این در فرآیند تولید آن نیز حدود 400 هزار لیتر آب و 4 هزار کیلووات برق مصرف می شود. می گویند بیش از نیمی از زباله های خشکی را که تولید می شود کاغذهای قابل بازیافت تشکیل می دهد. می گویند برآورد جهانی این است که بشر، محیط زیست را تا سال 2050 تخریب می کند. می گویند و می گویند و خیلی از ما هم می دانیم اما... بالاخره باید از جایی شروع کرد چرا که مصرف روزافزون کاغذ در کشور، ایجاد فشار بیش از حد بر جنگل‌ها و محیط زیست را به دنبال دارد و از بین رفتن درختان جنگل نیز مساوی با آلودگی هوا، .فرسایش خاک و هزاران تأثیر مخرب دیگر بر محیط ‌زیست است.\"\"\""
      ],
      "metadata": {
        "id": "ZfnQEB2v6glx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(sample_persian)"
      ],
      "metadata": {
        "id": "Mnxj66wW_qtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLg3ugqX_2MZ",
        "outputId": "cc1d0871-094b-48c3-92c9-97c5192356a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['این روزها دیگر کمتر به ارزش و تقدس درخت ها توجه می شود.',\n",
              " 'می گویند اوضاع مان در تولید کاغذ خوب نیست اما\\xa0اینجا زمین است، همان سیاره ای که خاطرات و سرگذشتمان در آن رقم خورده.',\n",
              " 'همان جا که روی آن متولد شدیم، رشد کردیم، بزرگ شدیم، درس خواندیم، برای هر لحظه زندگی نقشه کشیدیم، فکر کردیم و برای فرزندانمان برنامه ریختیم.',\n",
              " 'زمین همین جایی است که قرار است سال های سال و شاید قرن های قرن خانه نسل های بعد از ما باشد و آن ها هم بر همین خاکی زندگی کنند و در همین هوا تنفس کنند و از همین منابع زیست محیطی استفاده کنند که ما می کنیم.',\n",
              " 'اما این خانه بزرگ ما، احتیاج به محافظت دارد آن هم از طرف همه ما.',\n",
              " 'تا به حال شده از خودتان بپرسید این همه کاغذی که هرروز به شکل های مختلف؛ از روزنامه و دفتر و کتاب گرفته، تا دستمال کاغذی و بسته بندی های جور واجور، در دسترسمان است از کجا و چطور تأمین می شود؟',\n",
              " 'و تا به حال شده به حجم درخت هایی که به خاطر مصارف بی دلیل ما قطع می شود و ضربه ای که به اکوسیستم زده می شود، فکر کنید؟این روزها دیگر کمتر به ارزش و تقدس درخت ها توجه می شود.',\n",
              " 'می گویند اوضاع مان در تولید کاغذ خوب نیست اما مصرف مان خیلی زیاد و گاهی هم غیر منطقی است.',\n",
              " 'می گویند سالانه بیش از یک میلیون و 500 هزار تن کاغذ در کشور مصرف می شود.',\n",
              " 'می گویند برای تولید هر تُن کاغذ باید ۱۷ اصله درخت را قطع کرد و این یعنی قطع سالانه 25 میلیون و 500 هزار اصله درخت.',\n",
              " 'علاوه بر این در فرآیند تولید آن نیز حدود 400 هزار لیتر آب و 4 هزار کیلووات برق مصرف می شود.',\n",
              " 'می گویند بیش از نیمی از زباله های خشکی را که تولید می شود کاغذهای قابل بازیافت تشکیل می دهد.',\n",
              " 'می گویند برآورد جهانی این است که بشر، محیط زیست را تا سال 2050 تخریب می کند.',\n",
              " 'می گویند و می گویند و خیلی از ما هم می دانیم اما...',\n",
              " 'بالاخره باید از جایی شروع کرد چرا که مصرف روزافزون کاغذ در کشور، ایجاد فشار بیش از حد بر جنگل\\u200cها و محیط زیست را به دنبال دارد و از بین رفتن درختان جنگل نیز مساوی با آلودگی هوا، .فرسایش خاک و هزاران تأثیر مخرب دیگر بر محیط \\u200cزیست است.']"
            ]
          },
          "metadata": {},
          "execution_count": 685
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(sample_persian)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpMQ9xA___R5",
        "outputId": "0c04e005-97c3-4fb2-c7aa-b460c7610fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['این',\n",
              " 'روزها',\n",
              " 'دیگر',\n",
              " 'کمتر',\n",
              " 'به',\n",
              " 'ارزش',\n",
              " 'و',\n",
              " 'تقدس',\n",
              " 'درخت',\n",
              " 'ها',\n",
              " 'توجه',\n",
              " 'می',\n",
              " 'شود',\n",
              " '.',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'اوضاع',\n",
              " 'مان',\n",
              " 'در',\n",
              " 'تولید',\n",
              " 'کاغذ',\n",
              " 'خوب',\n",
              " 'نیست',\n",
              " 'اما\\xa0اینجا',\n",
              " 'زمین',\n",
              " 'است',\n",
              " '،',\n",
              " 'همان',\n",
              " 'سیاره',\n",
              " 'ای',\n",
              " 'که',\n",
              " 'خاطرات',\n",
              " 'و',\n",
              " 'سرگذشتمان',\n",
              " 'در',\n",
              " 'آن',\n",
              " 'رقم',\n",
              " 'خورده',\n",
              " '.',\n",
              " 'همان',\n",
              " 'جا',\n",
              " 'که',\n",
              " 'روی',\n",
              " 'آن',\n",
              " 'متولد',\n",
              " 'شدیم',\n",
              " '،',\n",
              " 'رشد',\n",
              " 'کردیم',\n",
              " '،',\n",
              " 'بزرگ',\n",
              " 'شدیم',\n",
              " '،',\n",
              " 'درس',\n",
              " 'خواندیم',\n",
              " '،',\n",
              " 'برای',\n",
              " 'هر',\n",
              " 'لحظه',\n",
              " 'زندگی',\n",
              " 'نقشه',\n",
              " 'کشیدیم',\n",
              " '،',\n",
              " 'فکر',\n",
              " 'کردیم',\n",
              " 'و',\n",
              " 'برای',\n",
              " 'فرزندانمان',\n",
              " 'برنامه',\n",
              " 'ریختیم',\n",
              " '.',\n",
              " 'زمین',\n",
              " 'همین',\n",
              " 'جایی',\n",
              " 'است',\n",
              " 'که',\n",
              " 'قرار',\n",
              " 'است',\n",
              " 'سال',\n",
              " 'های',\n",
              " 'سال',\n",
              " 'و',\n",
              " 'شاید',\n",
              " 'قرن',\n",
              " 'های',\n",
              " 'قرن',\n",
              " 'خانه',\n",
              " 'نسل',\n",
              " 'های',\n",
              " 'بعد',\n",
              " 'از',\n",
              " 'ما',\n",
              " 'باشد',\n",
              " 'و',\n",
              " 'آن',\n",
              " 'ها',\n",
              " 'هم',\n",
              " 'بر',\n",
              " 'همین',\n",
              " 'خاکی',\n",
              " 'زندگی',\n",
              " 'کنند',\n",
              " 'و',\n",
              " 'در',\n",
              " 'همین',\n",
              " 'هوا',\n",
              " 'تنفس',\n",
              " 'کنند',\n",
              " 'و',\n",
              " 'از',\n",
              " 'همین',\n",
              " 'منابع',\n",
              " 'زیست',\n",
              " 'محیطی',\n",
              " 'استفاده',\n",
              " 'کنند',\n",
              " 'که',\n",
              " 'ما',\n",
              " 'می',\n",
              " 'کنیم',\n",
              " '.',\n",
              " 'اما',\n",
              " 'این',\n",
              " 'خانه',\n",
              " 'بزرگ',\n",
              " 'ما',\n",
              " '،',\n",
              " 'احتیاج',\n",
              " 'به',\n",
              " 'محافظت',\n",
              " 'دارد',\n",
              " 'آن',\n",
              " 'هم',\n",
              " 'از',\n",
              " 'طرف',\n",
              " 'همه',\n",
              " 'ما',\n",
              " '.',\n",
              " 'تا',\n",
              " 'به',\n",
              " 'حال',\n",
              " 'شده',\n",
              " 'از',\n",
              " 'خودتان',\n",
              " 'بپرسید',\n",
              " 'این',\n",
              " 'همه',\n",
              " 'کاغذی',\n",
              " 'که',\n",
              " 'هرروز',\n",
              " 'به',\n",
              " 'شکل',\n",
              " 'های',\n",
              " 'مختلف',\n",
              " '؛',\n",
              " 'از',\n",
              " 'روزنامه',\n",
              " 'و',\n",
              " 'دفتر',\n",
              " 'و',\n",
              " 'کتاب',\n",
              " 'گرفته',\n",
              " '،',\n",
              " 'تا',\n",
              " 'دستمال',\n",
              " 'کاغذی',\n",
              " 'و',\n",
              " 'بسته',\n",
              " 'بندی',\n",
              " 'های',\n",
              " 'جور',\n",
              " 'واجور',\n",
              " '،',\n",
              " 'در',\n",
              " 'دسترسمان',\n",
              " 'است',\n",
              " 'از',\n",
              " 'کجا',\n",
              " 'و',\n",
              " 'چطور',\n",
              " 'تأمین',\n",
              " 'می',\n",
              " 'شود',\n",
              " '؟',\n",
              " 'و',\n",
              " 'تا',\n",
              " 'به',\n",
              " 'حال',\n",
              " 'شده',\n",
              " 'به',\n",
              " 'حجم',\n",
              " 'درخت',\n",
              " 'هایی',\n",
              " 'که',\n",
              " 'به',\n",
              " 'خاطر',\n",
              " 'مصارف',\n",
              " 'بی',\n",
              " 'دلیل',\n",
              " 'ما',\n",
              " 'قطع',\n",
              " 'می',\n",
              " 'شود',\n",
              " 'و',\n",
              " 'ضربه',\n",
              " 'ای',\n",
              " 'که',\n",
              " 'به',\n",
              " 'اکوسیستم',\n",
              " 'زده',\n",
              " 'می',\n",
              " 'شود',\n",
              " '،',\n",
              " 'فکر',\n",
              " 'کنید',\n",
              " '؟',\n",
              " 'این',\n",
              " 'روزها',\n",
              " 'دیگر',\n",
              " 'کمتر',\n",
              " 'به',\n",
              " 'ارزش',\n",
              " 'و',\n",
              " 'تقدس',\n",
              " 'درخت',\n",
              " 'ها',\n",
              " 'توجه',\n",
              " 'می',\n",
              " 'شود',\n",
              " '.',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'اوضاع',\n",
              " 'مان',\n",
              " 'در',\n",
              " 'تولید',\n",
              " 'کاغذ',\n",
              " 'خوب',\n",
              " 'نیست',\n",
              " 'اما',\n",
              " 'مصرف',\n",
              " 'مان',\n",
              " 'خیلی',\n",
              " 'زیاد',\n",
              " 'و',\n",
              " 'گاهی',\n",
              " 'هم',\n",
              " 'غیر',\n",
              " 'منطقی',\n",
              " 'است',\n",
              " '.',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'سالانه',\n",
              " 'بیش',\n",
              " 'از',\n",
              " 'یک',\n",
              " 'میلیون',\n",
              " 'و',\n",
              " '500',\n",
              " 'هزار',\n",
              " 'تن',\n",
              " 'کاغذ',\n",
              " 'در',\n",
              " 'کشور',\n",
              " 'مصرف',\n",
              " 'می',\n",
              " 'شود',\n",
              " '.',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'برای',\n",
              " 'تولید',\n",
              " 'هر',\n",
              " 'تُن',\n",
              " 'کاغذ',\n",
              " 'باید',\n",
              " '۱۷',\n",
              " 'اصله',\n",
              " 'درخت',\n",
              " 'را',\n",
              " 'قطع',\n",
              " 'کرد',\n",
              " 'و',\n",
              " 'این',\n",
              " 'یعنی',\n",
              " 'قطع',\n",
              " 'سالانه',\n",
              " '25',\n",
              " 'میلیون',\n",
              " 'و',\n",
              " '500',\n",
              " 'هزار',\n",
              " 'اصله',\n",
              " 'درخت',\n",
              " '.',\n",
              " 'علاوه',\n",
              " 'بر',\n",
              " 'این',\n",
              " 'در',\n",
              " 'فرآیند',\n",
              " 'تولید',\n",
              " 'آن',\n",
              " 'نیز',\n",
              " 'حدود',\n",
              " '400',\n",
              " 'هزار',\n",
              " 'لیتر',\n",
              " 'آب',\n",
              " 'و',\n",
              " '4',\n",
              " 'هزار',\n",
              " 'کیلووات',\n",
              " 'برق',\n",
              " 'مصرف',\n",
              " 'می',\n",
              " 'شود',\n",
              " '.',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'بیش',\n",
              " 'از',\n",
              " 'نیمی',\n",
              " 'از',\n",
              " 'زباله',\n",
              " 'های',\n",
              " 'خشکی',\n",
              " 'را',\n",
              " 'که',\n",
              " 'تولید',\n",
              " 'می',\n",
              " 'شود',\n",
              " 'کاغذهای',\n",
              " 'قابل',\n",
              " 'بازیافت',\n",
              " 'تشکیل',\n",
              " 'می',\n",
              " 'دهد',\n",
              " '.',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'برآورد',\n",
              " 'جهانی',\n",
              " 'این',\n",
              " 'است',\n",
              " 'که',\n",
              " 'بشر',\n",
              " '،',\n",
              " 'محیط',\n",
              " 'زیست',\n",
              " 'را',\n",
              " 'تا',\n",
              " 'سال',\n",
              " '2050',\n",
              " 'تخریب',\n",
              " 'می',\n",
              " 'کند',\n",
              " '.',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'و',\n",
              " 'می',\n",
              " 'گویند',\n",
              " 'و',\n",
              " 'خیلی',\n",
              " 'از',\n",
              " 'ما',\n",
              " 'هم',\n",
              " 'می',\n",
              " 'دانیم',\n",
              " 'اما',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " 'بالاخره',\n",
              " 'باید',\n",
              " 'از',\n",
              " 'جایی',\n",
              " 'شروع',\n",
              " 'کرد',\n",
              " 'چرا',\n",
              " 'که',\n",
              " 'مصرف',\n",
              " 'روزافزون',\n",
              " 'کاغذ',\n",
              " 'در',\n",
              " 'کشور',\n",
              " '،',\n",
              " 'ایجاد',\n",
              " 'فشار',\n",
              " 'بیش',\n",
              " 'از',\n",
              " 'حد',\n",
              " 'بر',\n",
              " 'جنگل\\u200cها',\n",
              " 'و',\n",
              " 'محیط',\n",
              " 'زیست',\n",
              " 'را',\n",
              " 'به',\n",
              " 'دنبال',\n",
              " 'دارد',\n",
              " 'و',\n",
              " 'از',\n",
              " 'بین',\n",
              " 'رفتن',\n",
              " 'درختان',\n",
              " 'جنگل',\n",
              " 'نیز',\n",
              " 'مساوی',\n",
              " 'با',\n",
              " 'آلودگی',\n",
              " 'هوا',\n",
              " '،',\n",
              " '.',\n",
              " 'فرسایش',\n",
              " 'خاک',\n",
              " 'و',\n",
              " 'هزاران',\n",
              " 'تأثیر',\n",
              " 'مخرب',\n",
              " 'دیگر',\n",
              " 'بر',\n",
              " 'محیط',\n",
              " '\\u200cزیست',\n",
              " 'است',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 686
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_xGEO5chO7q",
        "outputId": "78d77281-5ab4-4a0f-e2f1-c52c1f62c1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['این', 'روزها', 'دیگر', 'کمتر', 'به', 'ارزش', 'و', 'تقدس', 'درخت', 'ها', 'توجه', 'می', 'شود', '.', 'می', 'گویند', 'اوضاع', 'مان', 'در', 'تولید', 'کاغذ', 'خوب', 'نیست', 'اما\\xa0اینجا', 'زمین', 'است', '،', 'همان', 'سیاره', 'ای', 'که', 'خاطرات', 'و', 'سرگذشتمان', 'در', 'آن', 'رقم', 'خورده', '.', 'همان', 'جا', 'که', 'روی', 'آن', 'متولد', 'شدیم', '،', 'رشد', 'کردیم', '،', 'بزرگ', 'شدیم', '،', 'درس', 'خواندیم', '،', 'برای', 'هر', 'لحظه', 'زندگی', 'نقشه', 'کشیدیم', '،', 'فکر', 'کردیم', 'و', 'برای', 'فرزندانمان', 'برنامه', 'ریختیم', '.', 'زمین', 'همین', 'جایی', 'است', 'که', 'قرار', 'است', 'سال', 'های', 'سال', 'و', 'شاید', 'قرن', 'های', 'قرن', 'خانه', 'نسل', 'های', 'بعد', 'از', 'ما', 'باشد', 'و', 'آن', 'ها', 'هم', 'بر', 'همین', 'خاکی', 'زندگی', 'کنند', 'و', 'در', 'همین', 'هوا', 'تنفس', 'کنند', 'و', 'از', 'همین', 'منابع', 'زیست', 'محیطی', 'استفاده', 'کنند', 'که', 'ما', 'می', 'کنیم', '.', 'اما', 'این', 'خانه', 'بزرگ', 'ما', '،', 'احتیاج', 'به', 'محافظت', 'دارد', 'آن', 'هم', 'از', 'طرف', 'همه', 'ما', '.', 'تا', 'به', 'حال', 'شده', 'از', 'خودتان', 'بپرسید', 'این', 'همه', 'کاغذی', 'که', 'هرروز', 'به', 'شکل', 'های', 'مختلف', '؛', 'از', 'روزنامه', 'و', 'دفتر', 'و', 'کتاب', 'گرفته', '،', 'تا', 'دستمال', 'کاغذی', 'و', 'بسته', 'بندی', 'های', 'جور', 'واجور', '،', 'در', 'دسترسمان', 'است', 'از', 'کجا', 'و', 'چطور', 'تأمین', 'می', 'شود', '؟', 'و', 'تا', 'به', 'حال', 'شده', 'به', 'حجم', 'درخت', 'هایی', 'که', 'به', 'خاطر', 'مصارف', 'بی', 'دلیل', 'ما', 'قطع', 'می', 'شود', 'و', 'ضربه', 'ای', 'که', 'به', 'اکوسیستم', 'زده', 'می', 'شود', '،', 'فکر', 'کنید', '؟', 'این', 'روزها', 'دیگر', 'کمتر', 'به', 'ارزش', 'و', 'تقدس', 'درخت', 'ها', 'توجه', 'می', 'شود', '.', 'می', 'گویند', 'اوضاع', 'مان', 'در', 'تولید', 'کاغذ', 'خوب', 'نیست', 'اما', 'مصرف', 'مان', 'خیلی', 'زیاد', 'و', 'گاهی', 'هم', 'غیر', 'منطقی', 'است', '.', 'می', 'گویند', 'سالانه', 'بیش', 'از', 'یک', 'میلیون', 'و', '500', 'هزار', 'تن', 'کاغذ', 'در', 'کشور', 'مصرف', 'می', 'شود', '.', 'می', 'گویند', 'برای', 'تولید', 'هر', 'تُن', 'کاغذ', 'باید', '۱۷', 'اصله', 'درخت', 'را', 'قطع', 'کرد', 'و', 'این', 'یعنی', 'قطع', 'سالانه', '25', 'میلیون', 'و', '500', 'هزار', 'اصله', 'درخت', '.', 'علاوه', 'بر', 'این', 'در', 'فرآیند', 'تولید', 'آن', 'نیز', 'حدود', '400', 'هزار', 'لیتر', 'آب', 'و', '4', 'هزار', 'کیلووات', 'برق', 'مصرف', 'می', 'شود', '.', 'می', 'گویند', 'بیش', 'از', 'نیمی', 'از', 'زباله', 'های', 'خشکی', 'را', 'که', 'تولید', 'می', 'شود', 'کاغذهای', 'قابل', 'بازیافت', 'تشکیل', 'می', 'دهد', '.', 'می', 'گویند', 'برآورد', 'جهانی', 'این', 'است', 'که', 'بشر', '،', 'محیط', 'زیست', 'را', 'تا', 'سال', '2050', 'تخریب', 'می', 'کند', '.', 'می', 'گویند', 'و', 'می', 'گویند', 'و', 'خیلی', 'از', 'ما', 'هم', 'می', 'دانیم', 'اما', '.', '.', '.', 'بالاخره', 'باید', 'از', 'جایی', 'شروع', 'کرد', 'چرا', 'که', 'مصرف', 'روزافزون', 'کاغذ', 'در', 'کشور', '،', 'ایجاد', 'فشار', 'بیش', 'از', 'حد', 'بر', 'جنگل\\u200cها', 'و', 'محیط', 'زیست', 'را', 'به', 'دنبال', 'دارد', 'و', 'از', 'بین', 'رفتن', 'درختان', 'جنگل', 'نیز', 'مساوی', 'با', 'آلودگی', 'هوا', '،', '.', 'فرسایش', 'خاک', 'و', 'هزاران', 'تأثیر', 'مخرب', 'دیگر', 'بر', 'محیط', '\\u200cزیست', 'است', '.']\n",
            "427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Summarization**"
      ],
      "metadata": {
        "id": "uOK5QlgukrLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)recognize stopwords\n",
        "\n",
        "2)set highest value : Maximum Frequency of a word\n",
        "\n",
        "3)weight: Frequency of a word / highest value"
      ],
      "metadata": {
        "id": "jL3X9i1bntP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###preprocessing text"
      ],
      "metadata": {
        "id": "nzjXxxLpw4KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import string"
      ],
      "metadata": {
        "id": "mUlYnLtehjxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP nlp produces new and exciting results on a daily basis,and is a very large field. However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other\""
      ],
      "metadata": {
        "id": "OFT4gxr-qkcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ieb61J7urf5p",
        "outputId": "3beab094-6567-4daf-d398-a0bc8cb46772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP nlp produces new and exciting results on a daily basis,and is a very large field. However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 690
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#delete spaces\n",
        "text = re.sub(r'\\s+',' ', text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HpyU6SVmrqW3",
        "outputId": "9232a2c8-c6b4-4f19-f1ba-51ea46c80b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP nlp produces new and exciting results on a daily basis,and is a very large field. However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 691
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "h3HnOlk9sMLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943ba9b6-5924-49fb-9bf4-3ce67f9b523a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 692
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "# corpus is a sub-module that contains a variety of corpora (plural of corpus) for different languages and domains. These corpora are pre-processed and annotated with linguistic information, such as part-of-speech tags, named entities, and dependency structures."
      ],
      "metadata": {
        "id": "73Tr8KxLwNGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N9sOEBgA7NrZ",
        "outputId": "12b37b52-d8c1-432a-f2a0-0fe499d4d24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 694
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "def Preprocess(text):\n",
        "  formatted_text = text.lower()\n",
        "  tokens = []\n",
        "  for token in nltk.word_tokenize(formatted_text):\n",
        "    tokens.append(token)\n",
        "  \n",
        "  # delete stop words\n",
        "  tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
        "  #cancatinate all tokens together & convert list to string\n",
        "  formatted_text= ' '.join(element for element in tokens) #tokens is a list of strings, where each string represents a token (a word or a phrase) in the text that has been processed using some form of text preprocessing, such as tokenization, stemming, or lemmatization.\n",
        "  #This format is often used as input to machine learning models, which require text data to be represented as a sequence of tokens or vectors.\n",
        "  \n",
        "  return formatted_text"
      ],
      "metadata": {
        "id": "DYFlYxBHwRWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_text = Preprocess(text)\n",
        "formatted_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oEd16V5U3MGP",
        "outputId": "54c6ae81-d4c2-4f18-dccf-16988dba4178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nlp nlp produces new exciting results daily basis large field however worked hundreds companies insight team seen key practical applications come much frequently'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 696
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"من آموزش ها رو با دقت میبینم\""
      ],
      "metadata": {
        "id": "JDY39hlNBTqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = Normalizer()"
      ],
      "metadata": {
        "id": "HKP8YUN0BpIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_text = normalize.normalize(a)"
      ],
      "metadata": {
        "id": "FjpvfzmrCWML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_text\n",
        "# برای اینکه با زبون عربی قاتی نشه"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XYYyuAKgCV_Q",
        "outputId": "1a3febe2-0d6d-40e5-a0a6-b4a22b221a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'من آموزش\\u200cها رو با دقت میبینم'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 700
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Word frequently**"
      ],
      "metadata": {
        "id": "nt0ESzA462ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = nltk.FreqDist(nltk.word_tokenize(formatted_text))\n",
        "# formated_text is a string that contains a sequence of words, usually obtained by joining a list of tokens using the join() method, as shown in your previous code snippet.\n",
        "#  the FreqDist() function is a useful tool for analyzing the frequency distribution of words in a text corpus, and it can be used in various natural language processing tasks, such as text classification, topic modeling, and sentiment analysis.\n",
        "word_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjWJwKB_7B0k",
        "outputId": "a4163e6a-7482-4e6a-ac51-b16e4dbce4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'nlp': 2, 'produces': 1, 'new': 1, 'exciting': 1, 'results': 1, 'daily': 1, 'basis': 1, 'large': 1, 'field': 1, 'however': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 701
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq['nlp']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaJ9PWRW9lVt",
        "outputId": "b8f1b31a-79b2-4e4a-d610-7811a435f358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 702
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find unique word in sentences\n",
        "word_freq.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Ek9QkZ_myD",
        "outputId": "b35d1b9b-4295-4628-fdf9-f71fcaf8ab36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['nlp', 'produces', 'new', 'exciting', 'results', 'daily', 'basis', 'large', 'field', 'however', 'worked', 'hundreds', 'companies', 'insight', 'team', 'seen', 'key', 'practical', 'applications', 'come', 'much', 'frequently'])"
            ]
          },
          "metadata": {},
          "execution_count": 703
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_freq.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf8HU54k__UE",
        "outputId": "55549acb-990d-41b8-a344-67ae6cf2c9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 704
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define weight each sentences\n",
        "highest_values = max(word_freq.values())\n",
        "highest_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lqgfZbeALZE",
        "outputId": "4717e85f-0e3c-485c-d34a-f71b9b3db449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 705
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_freq.keys():\n",
        "  word_freq[word] - (word_freq[word] / highest_values)\n",
        "\n",
        "word_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDdULpQWBcOd",
        "outputId": "4099572f-beab-4dd8-dcc3-6863c0996919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'nlp': 2, 'produces': 1, 'new': 1, 'exciting': 1, 'results': 1, 'daily': 1, 'basis': 1, 'large': 1, 'field': 1, 'however': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 706
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sent tokenize\n",
        "text = nltk.sent_tokenize(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Obcu9q2LKuq",
        "outputId": "0b86831c-99e4-4fb0-d2ee-533f953ecfdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP nlp produces new and exciting results on a daily basis,and is a very large field.',\n",
              " 'However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other']"
            ]
          },
          "metadata": {},
          "execution_count": 707
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsO_Z9geMv7o",
        "outputId": "8d20b8fd-8f3e-4567-8b81-6c8ea6f9e3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 708
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**score each sentences**"
      ],
      "metadata": {
        "id": "ojOxXGvANVrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_sentences = {}\n",
        "for sent in text:\n",
        "  for word in nltk.word_tokenize(sent.lower()):\n",
        "    if sent not in score_sentences.keys():\n",
        "      score_sentences[sent] = word_freq[word]\n",
        "    else:\n",
        "      score_sentences[sent] += word_freq[word]  \n"
      ],
      "metadata": {
        "id": "eB5gpqfQND2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z44T_tTcSjCU",
        "outputId": "885caaa1-7499-47cc-abba-f08291645068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NLP nlp produces new and exciting results on a daily basis,and is a very large field.': 12,\n",
              " 'However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other': 13}"
            ]
          },
          "metadata": {},
          "execution_count": 710
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##why we need score_sentences in nlp?\n",
        "\n",
        "The score_sentences() function is a natural language processing (NLP) technique that assigns a score or a ranking to each sentence in a document, based on some predefined criteria or features. This function is used for text summarization, where the goal is to extract the most important information from a large text corpus and present it in a concise and readable format.\n",
        "\n",
        "Here are some reasons why we need to use score_sentences() in NLP:\n",
        "\n",
        "Summarization: By ranking the sentences in a document based on their relevance, importance, or novelty, score_sentences() can help to identify the most salient and informative sentences that capture the essence of the document. These sentences can be used to generate a summary of the document that is shorter and more concise than the original text.\n",
        "\n",
        "Information retrieval: By identifying the most relevant and informative sentences in a document, score_sentences() can help to improve the accuracy and efficiency of information retrieval systems, such as search engines, chatbots, and question-answering systems. These systems can use the ranked sentences as input to generate responses that match the user's query or intent.\n",
        "\n",
        "Text classification: By analyzing the linguistic features and patterns in each sentence, such as word frequency, syntax, sentiment, or topic, score_sentences() can help to classify the sentences into different categories, such as positive/negative, informative/opinionated, or factual/speculative. These categories can be used to train machine learning models that can automatically classify new sentences based on their features.\n",
        "\n",
        "Overall, score_sentences() is a useful NLP technique that can help to extract meaningful and relevant information from a large text corpus, and it can be used in various applications, such as summarization, information retrieval, and text classification.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1bxIhzGQ4aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "best_sentences = heapq.nlargest(1, score_sentences, key=score_sentences.get)\n",
        "best_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr6ZzaA4RuwF",
        "outputId": "2d207cd1-95e1-464a-dc69-78887133dcc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other']"
            ]
          },
          "metadata": {},
          "execution_count": 711
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = ' '.join(best_sentences)\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "knyA-J1DU-c4",
        "outputId": "ea5b4be8-0058-473d-d6e8-2dbfb7a331ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'However, having worked with hundreds of companies, the Insight team has seen a few key practical applications come up much more frequently than any other'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 712
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "heapq.nlargest() is a function in the Python heapq module that returns the n largest elements from an iterable object, such as a list or a tuple. Here's how the function works:\n",
        "\n",
        "heapq.nlargest(n, iterable[, key]) takes three arguments: n, which is an integer specifying the number of largest elements to return; iterable, which is an iterable object (such as a list, tuple, or generator) containing the elements to be searched; and key (optional), which is a function that takes an element of the iterable and returns a value that is used to compare the elements. By default, the elements are compared based on their natural ordering.\n",
        "\n",
        "The function creates a heap data structure from the iterable using the heapify() function from the heapq module. A heap is a binary tree-based data structure where the parent nodes are always larger than their child nodes (or smaller, depending on the implementation). This allows the largest (or smallest) elements to be quickly retrieved from the heap.\n",
        "\n",
        "The function uses the heappop() function to repeatedly extract the largest element from the heap and add it to a list of largest elements. This process is repeated n times, or until there are no more elements in the heap.\n",
        "\n",
        "The function returns the list of n largest elements, in descending order (i.e., from largest to smallest).\n",
        "\n",
        "Overall, heapq.nlargest() is a useful tool for finding the largest elements from a large dataset, especially when memory constraints are a concern, since it only stores a small portion of the dataset in memory at any given time. The function is commonly used in data analytics, machine learning, and numerical computing applications."
      ],
      "metadata": {
        "id": "9xjxtk1uWaBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pickle"
      ],
      "metadata": {
        "id": "W7s_3DPuHLHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pickle is a Python module that can be used to serialize (convert objects into byte streams) and deserialize (convert byte streams into objects) Python objects. The serialized byte stream can then be saved to a file or sent over a network, and later used to reconstruct the original object. This is useful when you want to store or transfer data that is more complex than a simple text file, such as Python objects with attributes and methods.\n",
        "Pickle is a useful Python tool that allows you to save your ML models, to minimise lengthy re-training and allow you to share, commit, and re-load pre-trained machine learning models. Most data scientists working in ML will use Pickle or Joblib to save their ML model for future use."
      ],
      "metadata": {
        "id": "yyL3CEaM753f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Stemming and Lemmatization**"
      ],
      "metadata": {
        "id": "BwohYbjPDGuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming and lemmatization are two important techniques in natural language processing (NLP) used to normalize and transform words to their base or root form.\n",
        "\n",
        "Stemming involves removing the suffixes (endings) from a word to get the base form or stem of the word. This technique uses a set of rules or algorithms to strip the suffixes from the words. The resulting stem may not always be a valid word in the language, but it is commonly used to reduce the dimensionality of the data and make it easier to process. For example, the stem of the word \"playing\" is \"play\", and the stem of the word \"played\" is also \"play\".\n",
        "\n",
        "Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma. This technique involves using a vocabulary or dictionary to map words to their base forms. The resulting lemma is always a valid word in the language, and this technique is generally more accurate than stemming. For example, the lemma of the word \"playing\" is \"play\", and the lemma of the word \"played\" is also \"play\". However, the lemma of the word \"was\" is \"be\", which is not the same as its stem.\n",
        "\n",
        "Both stemming and lemmatization are used to normalize text data and reduce its dimensionality, making it easier to analyze and process. The choice of which technique to use depends on the specific application and the desired level of accuracy. Stemming is faster and simpler but may result in some inaccuracies, while lemmatization is slower and more complex but generally more accurate."
      ],
      "metadata": {
        "id": "Y0iRmq6bDvAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *"
      ],
      "metadata": {
        "id": "1zPVPjowVzz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"connection\",\"connecting\",\"disconnect\",\"connect\",\"connected\"]"
      ],
      "metadata": {
        "id": "uvxHV-TzE0Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "HYMmEDMqFoTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  print(token + \" -> \" + stemmer.stem(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR6EAc_8Ft3R",
        "outputId": "e64ff7cb-03bd-44d4-953f-4c86b23e69cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connection -> connect\n",
            "connecting -> connect\n",
            "disconnect -> disconnect\n",
            "connect -> connect\n",
            "connected -> connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.lancaster import *"
      ],
      "metadata": {
        "id": "uR_qkFq9GAUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk.stem.lancaster is a module in the NLTK library that provides an implementation of the Lancaster stemming algorithm. This algorithm is a word stemming algorithm that removes the ends of words to obtain their base forms, or stems. Unlike some other stemming algorithms, the Lancaster algorithm is known for being very aggressive in its stemming, meaning that it can sometimes produce stems that are not actual words. Despite this, it is still a widely used algorithm in natural language processing tasks."
      ],
      "metadata": {
        "id": "HwgBhvaTHbWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import lancaster\n",
        "lancaster = LancasterStemmer()"
      ],
      "metadata": {
        "id": "v7Ap3ZYhHDLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"lovely\",\"decentralized\",\"better\",\"information\",\"disable\",\"did\"]"
      ],
      "metadata": {
        "id": "EepaSybvLfEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  print(token + \" -> \" + stemmer.stem(token))\n",
        "  print(token + \" -> \" + lancaster.stem(token))\n",
        "  print(\"\\n\")\n",
        "#compare 2 outputs  "
      ],
      "metadata": {
        "id": "Amllbdt4NlR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed80c375-dc43-4751-d311-f30be749627e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lovely -> love\n",
            "lovely -> lov\n",
            "\n",
            "\n",
            "decentralized -> decentr\n",
            "decentralized -> dec\n",
            "\n",
            "\n",
            "better -> better\n",
            "better -> bet\n",
            "\n",
            "\n",
            "information -> inform\n",
            "information -> inform\n",
            "\n",
            "\n",
            "disable -> disabl\n",
            "disable -> dis\n",
            "\n",
            "\n",
            "did -> did\n",
            "did -> did\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention! we have a problem! we can see stemmer doing over stemming\n",
        "###**Snowball**\n",
        "Snowball is a small string processing language for creating stemming algorithms for use in Information Retrieval, plus a collection of stemming algorithms implemented using it."
      ],
      "metadata": {
        "id": "_TvolxAvlngy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "rN1vrdDbePda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jRuZfQOCeZSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH6--m_GoGtj",
        "outputId": "511f4ac8-4417-4d00-b551-d55634ba9439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 723
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.find('corpora/wordnet.zip'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0mVkU8bpbN9",
        "outputId": "eb0cfebf-3398-4edd-8879-5d120dd812d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/nltk_data/corpora/wordnet.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python3 -m nltk.downloader wordnet\n",
        "# !unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora"
      ],
      "metadata": {
        "id": "mOsRFUUm1bTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  print(\"stem\" + \" -> \" + stemmer.stem(token))\n",
        "  print(\"lemma\" + \" -> \" + lemmatizer.lemmatize(token))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMzIZYNcnHco",
        "outputId": "d8f92972-128e-48e9-f228-e1f0680ecfdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem -> love\n",
            "lemma -> lovely\n",
            "\n",
            "\n",
            "stem -> decentr\n",
            "lemma -> decentralized\n",
            "\n",
            "\n",
            "stem -> better\n",
            "lemma -> better\n",
            "\n",
            "\n",
            "stem -> inform\n",
            "lemma -> information\n",
            "\n",
            "\n",
            "stem -> disabl\n",
            "lemma -> disable\n",
            "\n",
            "\n",
            "stem -> did\n",
            "lemma -> did\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lema must understand the role of this word in the sentence, it's define part of speech.\n",
        "Part of speech (POS) refers to the grammatical category of words based on their function within sentences. POS categorizes words into classes such as nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, and interjections. Identifying the part of speech of words in a sentence can help in understanding the meaning of the sentence and is useful for many natural language processing tasks such as text classification, sentiment analysis, and language translation. In NLP, POS tagging is the process of automatically assigning POS tags to words in a text corpus."
      ],
      "metadata": {
        "id": "myU8Pa5Eccbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpXfMI5Ahabc",
        "outputId": "aeab10cb-27a6-4463-dd71-bface03e3b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 727
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  print(\"stem\" + \" -> \" + stemmer.stem(token))\n",
        "  print(\"lemma\" + \" -> \" + lemmatizer.lemmatize(token))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYTuk6wcfESC",
        "outputId": "cbbff566-114a-4c6d-895f-b77f8742e3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem -> love\n",
            "lemma -> lovely\n",
            "\n",
            "\n",
            "stem -> decentr\n",
            "lemma -> decentralized\n",
            "\n",
            "\n",
            "stem -> better\n",
            "lemma -> better\n",
            "\n",
            "\n",
            "stem -> inform\n",
            "lemma -> information\n",
            "\n",
            "\n",
            "stem -> disabl\n",
            "lemma -> disable\n",
            "\n",
            "\n",
            "stem -> did\n",
            "lemma -> did\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  print(\"stem\" + \" -> \" + stemmer.stem(token))\n",
        "  print(\"lemma\" + \" -> \" + lemmatizer.lemmatize(token, pos= \"a\"))\n",
        "  print(\"\\n\")\n",
        "\n",
        " # a means adjectives\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9t84WUnccLj",
        "outputId": "79ee3ae1-fdd5-42e2-8de3-56114c387c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem -> love\n",
            "lemma -> lovely\n",
            "\n",
            "\n",
            "stem -> decentr\n",
            "lemma -> decentralized\n",
            "\n",
            "\n",
            "stem -> better\n",
            "lemma -> good\n",
            "\n",
            "\n",
            "stem -> inform\n",
            "lemma -> information\n",
            "\n",
            "\n",
            "stem -> disabl\n",
            "lemma -> disable\n",
            "\n",
            "\n",
            "stem -> did\n",
            "lemma -> did\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  print(token + \" -> \" + stemmer.stem(token))\n",
        "  print(token + \" -> \" + lemmatizer.lemmatize(token, pos= \"v\"))\n",
        "  print(\"\\n\")\n",
        "\n",
        "# v means verb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uefgyxm6dyPs",
        "outputId": "eea830c0-50c8-4553-cc5c-5b7461ee8ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lovely -> love\n",
            "lovely -> lovely\n",
            "\n",
            "\n",
            "decentralized -> decentr\n",
            "decentralized -> decentralize\n",
            "\n",
            "\n",
            "better -> better\n",
            "better -> better\n",
            "\n",
            "\n",
            "information -> inform\n",
            "information -> information\n",
            "\n",
            "\n",
            "disable -> disabl\n",
            "disable -> disable\n",
            "\n",
            "\n",
            "did -> did\n",
            "did -> do\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#persian\n",
        "per_tokens = [\"کتابی\",\"کتابم\",\"کتاب ها\",\"رفتی\",\"می روم\",\"رفت\"]"
      ],
      "metadata": {
        "id": "0x2a5s6Ud2om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer()"
      ],
      "metadata": {
        "id": "lmCKBunh8FcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = Lemmatizer()"
      ],
      "metadata": {
        "id": "GTijHbsc8OFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()"
      ],
      "metadata": {
        "id": "pz0sKgjE8WZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in per_tokens:\n",
        "  print(\"stem\" + \" -> \" + stemmer.stem(token))\n",
        "  print(\"lemma\" + \" -> \" + lemmatizer.lemmatize(token, pos= \"a\"))\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-gUsbQa8c4N",
        "outputId": "19a04e70-884d-46cb-9e0f-46f1a65d7e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem -> کتاب\n",
            "lemma -> کتاب\n",
            "\n",
            "\n",
            "stem -> کتاب\n",
            "lemma -> کتاب\n",
            "\n",
            "\n",
            "stem -> کتاب \n",
            "lemma -> کتاب ها\n",
            "\n",
            "\n",
            "stem -> رفت\n",
            "lemma -> رفتی\n",
            "\n",
            "\n",
            "stem -> می رو\n",
            "lemma -> می روم\n",
            "\n",
            "\n",
            "stem -> رف\n",
            "lemma -> رف\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Bag of words**\n",
        "Bag of words is a common technique used in natural language processing (NLP) to represent text data as numerical features. The idea is to create a \"bag\" (or set) of all the unique words in a corpus of text, and then count the frequency of each word in a given document. This results in a vector representation of the document, where each element corresponds to the count of a specific word in the bag.\n",
        "Bag of words is a simple and effective technique for feature extraction in NLP, but it has limitations. For example, it ignores the order and context of words in a sentence, and it does not account for the meaning of words or phrases. More advanced techniques, such as word embeddings and deep learning models, have been developed to address these limitations."
      ],
      "metadata": {
        "id": "3g1f0gZ-9dhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TF-IDF**\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used in natural language processing to evaluate the importance of a word in a document or corpus. It works by calculating the product of two values: term frequency (TF) and inverse document frequency (IDF).\n",
        "\n",
        "TF refers to how often a specific term or word appears in a document. The more often a word appears, the higher its TF score will be. IDF refers to how common a word is across all documents in the corpus. The more common a word is, the lower its IDF score will be.\n",
        "\n",
        "The TF-IDF score for a given term in a document is calculated as the product of its TF and IDF scores. This score gives an indication of how important a term is to a particular document compared to the other documents in the corpus.\n",
        "\n",
        "The use of TF-IDF is a common technique in various NLP tasks such as information retrieval, document classification, and sentiment analysis. It helps in reducing the impact of common words in a text while highlighting the important ones, thus enabling the creation of more accurate models."
      ],
      "metadata": {
        "id": "TviR6g7U_FgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ],
      "metadata": {
        "id": "Ww41OpbH8leH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = \"\"\"A major drawback of statistical methods is that they require elaborate feature engineering. \n",
        "Since the early 2010s, the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. \n",
        "Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing).\n",
        "In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. \n",
        "For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). \n",
        "Latest works tend to use non-technical structure of a given task to build proper neural network\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A9LEThh-KcI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning text\n",
        "sentences = sent_tokenize(t)\n",
        "stemmer = PorterStemmer()\n",
        "corpus = []\n",
        "for sent in sentences:\n",
        "    review = re.sub(\"[^a-zA-Z]\", \" \", sent)\n",
        "    review = re.sub(\"\\b[a-zA-Z]\\b\", \" \", review)\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = \" \".join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "8UHpk1dwKfr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TwbQwF5MPyIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scikit-learn**\n",
        "Scikit-learn is a popular open-source machine learning library for Python. It provides a range of tools for supervised and unsupervised learning, including classification, regression, clustering, and dimensionality reduction via a consistent interface.\n",
        "\n",
        "Scikit-learn is built on top of NumPy, SciPy, and matplotlib, and it is designed to work with these and other scientific computing libraries. It provides efficient implementations of a large number of common machine learning algorithms, as well as tools for preprocessing data, model selection, and evaluation.\n",
        "\n",
        "Some of the popular algorithms available in scikit-learn include linear regression, logistic regression, decision trees, random forests, support vector machines, k-nearest neighbors, k-means clustering, and principal component analysis. Scikit-learn also includes utilities for handling text data, such as feature extraction and text classification.\n",
        "\n",
        "Overall, scikit-learn is a powerful and versatile library for machine learning in Python, suitable for both beginners and experienced users."
      ],
      "metadata": {
        "id": "XNrQ_UWSP10T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature_extraction in nlp**\n",
        "In NLP, feature extraction refers to the process of transforming raw text data into numerical features that can be used by machine learning algorithms to train models. The goal of feature extraction is to convert the text data into a format that can be easily analyzed and processed by the machine learning algorithms.\n",
        "\n",
        "In scikit-learn, feature extraction is performed using a variety of techniques such as CountVectorizer, TfidfVectorizer, HashingVectorizer, and others. These techniques convert text data into numerical vectors that can be used for modeling.\n",
        "\n",
        "CountVectorizer, for example, converts text into a bag-of-words representation, where each document is represented as a vector of word counts. TfidfVectorizer, on the other hand, assigns a weight to each word in the bag-of-words representation based on its importance in the corpus. Other techniques, like HashingVectorizer, use hashing functions to map words to fixed-length vectors.\n",
        "\n",
        "In addition to these techniques, scikit-learn also provides tools for feature selection and dimensionality reduction, which are used to reduce the number of features in the data and improve the efficiency and accuracy of the models."
      ],
      "metadata": {
        "id": "FX2cItALQhAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What are the problems with bag of words?**\n",
        "The Bag-of-Words (BoW) model is a simple and commonly used representation of text in Natural Language Processing (NLP). In BoW, a document is represented as a bag of words, without considering grammar and word order. While BoW has several advantages such as simplicity, efficiency, and ease of implementation, it also has some limitations or problems:\n",
        "\n",
        "Loss of word order: Since BoW only considers the frequency of words in a document and disregards the word order, it may not be able to capture the meaning of a sentence accurately.\n",
        "\n",
        "Large feature space: BoW results in a sparse matrix, which means that there are a large number of features, most of which have very low frequency. This can lead to overfitting and difficulties in modeling.\n",
        "\n",
        "Out-of-vocabulary words: BoW does not consider words that are not present in the vocabulary or the corpus used for creating the model. This can lead to loss of important information and affect the model's performance.\n",
        "\n",
        "Meaningless words: BoW treats all words equally, without considering their semantic meaning or context. This means that common and meaningless words such as \"the\", \"and\", and \"a\" have the same importance as more meaningful words.\n",
        "\n",
        "To address these problems, various techniques have been developed such as n-grams, TF-IDF, and word embeddings, which aim to capture the context and meaning of words in a more efficient and accurate way.\n"
      ],
      "metadata": {
        "id": "O-3CqU8BSH5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**vectorization**\n",
        "In natural language processing, vectorization is the process of converting text into numerical vectors that can be used as input to machine learning models. Vectorization allows algorithms to understand and process the textual data.\n",
        "\n",
        "There are various techniques for vectorization in NLP such as:\n",
        "\n",
        "Bag of Words (BoW): In this technique, each document is represented as a bag of words, where the frequency of each word in the document is considered as a feature.\n",
        "\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF): This technique considers not only the frequency of a word in a document but also its frequency across all documents. Words that occur frequently across documents are given less weightage.\n",
        "\n",
        "Word Embedding: This technique represents words in a high-dimensional vector space where words with similar meaning are close to each other. Word embeddings are learned using algorithms like Word2Vec, GloVe, and fastText.\n",
        "\n",
        "Doc2Vec: This technique extends Word2Vec to represent documents as vectors in a high-dimensional vector space. The model learns to represent documents as continuous vectors by predicting words in a document.\n",
        "\n",
        "Count Vectors with Hashing Trick: This technique converts text into fixed-length vectors, which are hashed values of the words. It is a memory-efficient technique, but it can lead to hash collisions.\n",
        "\n",
        "The choice of vectorization technique depends on the specific problem and the available data."
      ],
      "metadata": {
        "id": "ZnCxL51NUKah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "bow = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()\n",
        "tfidf = tf.fit_transform(corpus).toarray()\n"
      ],
      "metadata": {
        "id": "ZrDmgDIaKn4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}